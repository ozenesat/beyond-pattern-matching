#!/usr/bin/env python3
"""
Brainfuck Evolution: General Function Calculator
A genetic algorithm implementation for evolving Brainfuck programs.
"""

import random
import time
import json
import os
import sys
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple, Callable
from dataclasses import dataclass, asdict

# Import dependencies from local copies
from brainfuck import BrainfuckInterpreter
from genome_repository import GenomeRepository, get_global_repository


@dataclass
class TestResult:
    """Result of testing an individual on a single test case."""
    input_val: int
    expected: int
    actual: int
    fitness: float
    error: Optional[str] = None


@dataclass
class EvolutionConfig:
    """Configuration parameters for the evolution process."""
    population_size: int = 50
    mutation_rate: float = 0.1
    max_program_length: int = 50
    execution_timeout_ms: int = 100
    test_cases: List[int] = None
    expected_outputs: List[int] = None
    input_output_mapping: Dict[int, int] = None
    generation_delay_ms: int = 100
    elite_ratio: float = 0.1
    tournament_size: int = 3
    crossover_rate: float = 0.7
    max_generations: int = 1000
    target_fitness: float = 100.0
    function_name: str = None  # Description of the target function

    # Lambda function support for dynamic test generation
    target_function: Optional[Callable[[int], int]] = None  # Lambda function: lambda x: x + 1
    input_range: Tuple[int, int] = (0, 20)     # Range for sampling inputs (min, max)
    samples_per_generation: int = 6            # Number of test cases to sample per generation
    validation_samples: int = 10               # Number of validation cases for PERFECT detection
    max_output_value: int = 255                # Cap outputs at 255 for Brainfuck compatibility
    
    # Dynamic test case sampling (legacy support)
    input_pool_mapping: Dict[int, int] = None  # Large pool of input-output pairs (legacy)
    use_dynamic_sampling: bool = False         # Whether to use sampling instead of fixed tests
    include_previous_failures: bool = True     # Whether to bias toward previously failed cases
    
    # Validation set for PERFECT detection (prevents false positives)
    validation_set_mapping: Dict[int, int] = None  # Separate validation inputs for final check
    validation_set_size: int = 10                  # Size of validation set
    
    # Sampling improvements
    failure_count_decay: float = 0.95              # Decay factor for failure counts each generation
    use_sampling_without_replacement: bool = True  # Sample without replacement to avoid duplicates
    
    # Population diversity management
    max_duplicates_per_genome: int = 3             # Allow up to N copies of same genome (preserves selection pressure)
    strict_uniqueness: bool = False                # If False, use capped duplicates instead

    # Mutation annealing
    initial_mutation_rate: float = 0.25  # Starting mutation rate
    final_mutation_rate: float = 0.05    # Ending mutation rate
    anneal_mutations: bool = True        # Whether to gradually reduce mutation rate

    # Migration options
    migration_rate: float = 0.05  # Fraction of population to replace during migration
    migration_frequency: int = 10  # Migration occurs every N generations

    # Genome repository options
    use_genome_repository: bool = True  # Whether to use stored genomes for seeding
    save_successful_genomes: bool = True  # Whether to save successful genomes
    repository_seed_ratio: float = 0.3  # Fraction of population to seed from repository
    min_accuracy_to_save: float = 100.0  # Minimum accuracy to save to repository

    def __post_init__(self):
        # Handle different ways of specifying test cases and expected outputs
        
        # NEW: Lambda function support - test cases will be generated dynamically
        if self.target_function is not None:
            # Lambda function mode - test cases will be generated by _resample_from_lambda()
            # Initialize with empty values, will be populated during engine initialization
            self.test_cases = []
            self.expected_outputs = []
            self.input_output_mapping = {}
            return
            
        # LEGACY: Handle traditional test case specification
        if self.use_dynamic_sampling:
            # For dynamic sampling, we need an input pool
            if self.input_pool_mapping is None:
                raise ValueError("input_pool_mapping must be specified when using dynamic sampling")
            # Set up initial test cases from pool (will be resampled each generation)
            pool_items = list(self.input_pool_mapping.items())
            sample_size = min(self.samples_per_generation, len(pool_items))
            sampled_items = random.sample(pool_items, sample_size)
            self.test_cases = [item[0] for item in sampled_items]
            self.expected_outputs = [item[1] for item in sampled_items]
            self.input_output_mapping = dict(sampled_items)
        elif self.input_output_mapping is not None:
            # Use explicit input-output mapping
            self.test_cases = list(self.input_output_mapping.keys())
            self.expected_outputs = list(self.input_output_mapping.values())
        elif self.test_cases is not None and self.expected_outputs is not None:
            # Use separate lists for inputs and outputs
            if len(self.test_cases) != len(self.expected_outputs):
                raise ValueError("test_cases and expected_outputs must have the same length")
        elif self.test_cases is None and self.expected_outputs is None:
            raise ValueError("Either target_function, test_cases and expected_outputs, or input_output_mapping/input_pool_mapping must be specified")
        else:
            raise ValueError("Either target_function, test_cases and expected_outputs, or input_output_mapping/input_pool_mapping must be specified")

        # Create mapping for easy lookup
        if self.input_output_mapping is None:
            self.input_output_mapping = dict(zip(self.test_cases, self.expected_outputs))


class Individual:
    """Represents a single Brainfuck program in the population."""

    def __init__(self, code: str = "", max_length: int = 50):
        self.max_length = max_length
        self.code = code or self._generate_random()
        self.fitness = 0.0
        self.accuracy = 0.0  # Percentage of test cases that are exactly correct
        self.test_results: List[TestResult] = []
        self.generation_created = 0
    
    def _generate_random(self) -> str:
        """Generate a structured Brainfuck program: ,<body>."""
        # Use instance max_length if available, otherwise default
        max_program_length = getattr(self, 'max_length', 50)
        
        # Enforce structure: , + <body> + .
        # Body length is total length - 2 (for , and .)
        body_length = max(1, random.randint(1, max_program_length - 2))
        
        # Generate body with basic operations only (no extra inputs for doubling)
        body_commands = '><+-[]'
        body = ''
        bracket_depth = 0
        
        for _ in range(body_length):
            # Bias towards closing brackets if we have open ones
            if bracket_depth > 0 and random.random() < 0.3:
                cmd = ']'
                bracket_depth -= 1
            else:
                cmd = random.choice(body_commands)
                if cmd == '[':
                    bracket_depth += 1
            body += cmd
        
        # Close any remaining brackets
        body += ']' * bracket_depth
        
        # Construct final program: ,<body>.
        code = ',' + body + '.'
        
        return code
    
    def mutate(self, config: EvolutionConfig) -> None:
        """Apply structure-preserving mutation to ,<body>. programs."""
        if random.random() > config.mutation_rate:
            return

        # Validate structure first
        if not self.code.startswith(',') or not self.code.endswith('.') or len(self.code) < 3:
            # Fix broken structure
            if len(self.code) >= 3:
                self.code = ',' + self.code[1:-1] + '.'
            else:
                self.code = ',+.'  # Minimal valid program
            return

        # Extract body (everything between first ',' and last '.')
        body = self.code[1:-1]
        if len(body) == 0:
            body = '+'  # Ensure non-empty body
            
        body_commands = '><+-[]'  # No extra inputs in body for doubling
        max_attempts = 10
        
        for attempt in range(max_attempts):
            new_body = body
            mutation_type = random.random()

            if mutation_type < 0.4 and len(new_body) > 0:
                # Point mutation within body
                pos = random.randint(0, len(new_body) - 1)
                current_char = new_body[pos]
                
                if current_char in '[]':
                    # Replace bracket carefully
                    new_cmd = random.choice('><+-')
                else:
                    new_cmd = random.choice(body_commands)
                
                new_body = new_body[:pos] + new_cmd + new_body[pos + 1:]
                
            elif mutation_type < 0.7:
                # Insertion within body
                pos = random.randint(0, len(new_body))
                new_cmd = random.choice(body_commands)
                new_body = new_body[:pos] + new_cmd + new_body[pos:]
                
            elif mutation_type < 0.85 and len(new_body) > 1:
                # Deletion within body
                pos = random.randint(0, len(new_body) - 1)
                new_body = new_body[:pos] + new_body[pos + 1:]
                
            else:
                # Insert useful chunks (load from macro repository if available)
                useful_chunks = self._get_useful_chunks()
                chunk = random.choice(useful_chunks)
                pos = random.randint(0, len(new_body))
                new_body = new_body[:pos] + chunk + new_body[pos:]

            # Ensure body isn't empty
            if len(new_body) == 0:
                new_body = '+'
                
            # Check if body has valid brackets and length is reasonable
            full_code = ',' + new_body + '.'
            if (self._is_valid_brackets(full_code) and 
                len(full_code) <= config.max_program_length):
                self.code = full_code
                return
            
        # If all attempts failed, make a safe mutation
        if len(body) > 0:
            # Replace one non-bracket character in body
            safe_positions = [i for i, c in enumerate(body) if c not in '[]']
            if safe_positions:
                pos = random.choice(safe_positions)
                new_char = random.choice('><+-')
                new_body = body[:pos] + new_char + body[pos + 1:]
                self.code = ',' + new_body + '.'
    
    def _get_useful_chunks(self) -> List[str]:
        """Get useful chunks from macro repository or defaults."""
        try:
            # Try to load from macro repository
            import json
            with open('primitive-evolution/blog_demos/macro_repository.json', 'r') as f:
                repository = json.load(f)
            useful_chunks = repository.get('useful_chunks', [])
            if useful_chunks:
                return useful_chunks
        except (FileNotFoundError, json.JSONDecodeError, KeyError):
            pass
        
        # Fall back to single character commands if no macros available
        body_commands = '><+-[]'
        return [x for x in body_commands]
    
    def _is_valid_brackets(self, code: str) -> bool:
        """Check if bracket structure is valid without modifying."""
        depth = 0
        for char in code:
            if char == '[':
                depth += 1
            elif char == ']':
                if depth == 0:
                    return False
                depth -= 1
        return depth == 0

    def _fix_brackets(self, code: str) -> str:
        """Fix bracket matching in the code."""
        depth = 0
        fixed = ''
        
        for char in code:
            if char == '[':
                depth += 1
                fixed += char
            elif char == ']' and depth > 0:
                depth -= 1
                fixed += char
            elif char != ']':
                fixed += char
        
        # Close remaining brackets
        fixed += ']' * depth
        
        return fixed
    
    def _find_bracket_blocks(self, code: str) -> List[Tuple[int, int]]:
        """Find positions of balanced bracket blocks [...]."""
        blocks = []
        stack = []
        
        for i, char in enumerate(code):
            if char == '[':
                stack.append(i)
            elif char == ']' and stack:
                start = stack.pop()
                blocks.append((start, i))
        
        return blocks
    
    def clone(self) -> 'Individual':
        """Create a copy of this individual."""
        clone = Individual(self.code, max_length=getattr(self, 'max_length', 50))
        clone.fitness = self.fitness
        clone.accuracy = self.accuracy
        clone.generation_created = self.generation_created
        clone.test_results = self.test_results.copy()  # Copy test results!
        return clone


class EvolutionEngine:
    """Main evolution engine for Brainfuck programs."""
    
    def __init__(self, config: EvolutionConfig = None):
        self.config = config or EvolutionConfig()
        self.population: List[Individual] = []
        self.generation = 0
        self.running = False
        self.interpreter = BrainfuckInterpreter()
        self.best_ever: Optional[Individual] = None
        self.best_accuracy_ever = 0.0
        self.evolution_log: List[str] = []
        self.genome_repository = get_global_repository() if config.use_genome_repository else None
        
        # Initialize mutation rate for annealing
        if self.config.anneal_mutations:
            self.current_mutation_rate = self.config.initial_mutation_rate
        else:
            self.current_mutation_rate = self.config.mutation_rate
            
        # For tracking test case difficulty
        self.failed_cases_history: Dict[int, float] = {}  # input -> failure count (with decay)
        self.used_test_cases_this_generation: set = set()  # Track cases used this generation
        
        # Set up validation set and initial test cases
        if self.config.target_function is not None:
            # Using lambda function - set up initial test cases and validation
            self._resample_from_lambda()  # Set initial test cases
            self._setup_validation_set()  # Set up validation
        elif self.config.use_dynamic_sampling and self.config.input_pool_mapping:
            # Using legacy input pool sampling
            self._setup_validation_set()

    def initialize_population(self) -> None:
        """Initialize the population with random individuals and genome repository seeding."""
        self.population = []
        seeded_count = 0

        # Seed from genome repository if enabled
        if self.genome_repository and self.config.use_genome_repository:
            seed_count = int(self.config.population_size * self.config.repository_seed_ratio)
            seed_genomes = self.genome_repository.export_for_seeding(
                function_name=self.config.function_name,
                limit=seed_count
            )

            for genome_code in seed_genomes:
                individual = Individual(genome_code)
                individual.generation_created = 0
                self.population.append(individual)
                seeded_count += 1

            if seeded_count > 0:
                self.log(f"Seeded {seeded_count} individuals from genome repository")

        # Fill remaining population with random individuals
        while len(self.population) < self.config.population_size:
            individual = Individual(max_length=self.config.max_program_length)
            individual.generation_created = 0
            self.population.append(individual)

        self.generation = 0
        self.best_ever = None
        self.best_accuracy_ever = 0.0

        random_count = len(self.population) - seeded_count
        self.log(f"Population initialized: {seeded_count} seeded, {random_count} random individuals")
    
    def resample_test_cases(self) -> None:
        """Resample test cases from lambda function or input pool for this generation."""
        
        # Check if using lambda function sampling
        if self.config.target_function is not None:
            self._resample_from_lambda()
            return
            
        # Legacy: Use input pool sampling
        if not self.config.use_dynamic_sampling:
            return
        
        # Decay failure counts to prevent permanent bias
        for inp in self.failed_cases_history:
            self.failed_cases_history[inp] *= self.config.failure_count_decay
            
        # Clear this generation's usage tracking
        self.used_test_cases_this_generation = set()
            
        pool_items = list(self.config.input_pool_mapping.items())
        sample_size = min(self.config.samples_per_generation, len(pool_items))
        
        if self.config.include_previous_failures and self.failed_cases_history:
            # Bias sampling toward previously difficult cases
            weights = []
            for inp, _ in pool_items:
                # Higher weight for cases that failed more often (with decay)
                failure_count = self.failed_cases_history.get(inp, 0.0)
                weight = 1.0 + failure_count * 2.0  # Base weight + failure bonus
                weights.append(weight)
            
            if self.config.use_sampling_without_replacement:
                # Weighted sampling without replacement
                sampled_items = []
                available_items = pool_items.copy()
                available_weights = weights.copy()
                
                for _ in range(sample_size):
                    if not available_items:
                        break
                    # Choose one item based on weights
                    chosen_idx = random.choices(range(len(available_items)), weights=available_weights, k=1)[0]
                    sampled_items.append(available_items.pop(chosen_idx))
                    available_weights.pop(chosen_idx)
            else:
                # Weighted sampling with replacement (original behavior)
                sampled_items = random.choices(pool_items, weights=weights, k=sample_size)
        else:
            # Uniform sampling
            if self.config.use_sampling_without_replacement:
                sampled_items = random.sample(pool_items, sample_size)
            else:
                sampled_items = [random.choice(pool_items) for _ in range(sample_size)]
        
        # Track which test cases we're using this generation
        for inp, _ in sampled_items:
            self.used_test_cases_this_generation.add(inp)
        
        # Update config with new test cases
        self.config.test_cases = [item[0] for item in sampled_items]
        self.config.expected_outputs = [item[1] for item in sampled_items]
        self.config.input_output_mapping = dict(sampled_items)
        
        self.log(f"Resampled test cases: {self.config.test_cases} → {self.config.expected_outputs}")
    
    def _resample_from_lambda(self) -> None:
        """Sample test cases from lambda function with bias toward difficult cases."""
        min_input, max_input = self.config.input_range
        
        # Decay failure counts to prevent permanent bias
        for inp in self.failed_cases_history:
            self.failed_cases_history[inp] *= self.config.failure_count_decay
            
        # Clear this generation's usage tracking
        self.used_test_cases_this_generation = set()
        
        # Generate possible inputs in range
        possible_inputs = list(range(min_input, max_input + 1))
        
        # Filter out inputs that would cause output > max_output_value
        valid_inputs = []
        for inp in possible_inputs:
            try:
                output = self.config.target_function(inp)
                if 0 <= output <= self.config.max_output_value:
                    valid_inputs.append(inp)
            except:
                # Skip inputs that cause errors in the lambda function
                continue
        
        if len(valid_inputs) == 0:
            raise ValueError(f"No valid inputs found in range {self.config.input_range} that produce outputs ≤ {self.config.max_output_value}")
        
        sample_size = min(self.config.samples_per_generation, len(valid_inputs))
        
        # Bias toward previously failed cases if we have history
        if self.config.include_previous_failures and self.failed_cases_history:
            weights = []
            for inp in valid_inputs:
                failure_count = self.failed_cases_history.get(inp, 0.0)
                weight = 1.0 + failure_count * 2.0  # Base weight + failure bonus
                weights.append(weight)
            
            sampled_inputs = random.choices(valid_inputs, weights=weights, k=sample_size)
        else:
            # Uniform sampling
            sampled_inputs = random.sample(valid_inputs, min(sample_size, len(valid_inputs)))
        
        # Generate corresponding outputs using lambda function
        sampled_items = []
        for inp in sampled_inputs:
            try:
                output = self.config.target_function(inp)
                if 0 <= output <= self.config.max_output_value:
                    sampled_items.append((inp, output))
            except:
                # Skip if function fails
                continue
        
        # Track which test cases we're using this generation
        for inp, _ in sampled_items:
            self.used_test_cases_this_generation.add(inp)
        
        # Update config with new test cases
        self.config.test_cases = [item[0] for item in sampled_items]
        self.config.expected_outputs = [item[1] for item in sampled_items]
        self.config.input_output_mapping = dict(sampled_items)
        
        self.log(f"Lambda sampled test cases: {self.config.test_cases} → {self.config.expected_outputs}")
    
    def _compute_static_penalties(self, code: str) -> float:
        """Compute structural penalties once per individual, not per test."""
        penalty = 0.0
        if not code.startswith(','): 
            penalty += 50.0
        if not code.endswith('.'): 
            penalty += 50.0
        if code.count('.') != 1: 
            penalty += 30.0 * abs(code.count('.') - 1)
        
        # For doubling, prefer exactly one input read
        comma_count = code.count(',')
        if comma_count != 1:
            penalty += 20.0 * abs(comma_count - 1)
        
        return penalty

    def evaluate_fitness(self, individual: Individual) -> float:
        """Evaluate the fitness of an individual against all test cases."""
        total_fitness = 0.0
        correct_count = 0
        individual.test_results = []
        
        # Compute structural penalties once per individual
        static_penalty = self._compute_static_penalties(individual.code)

        for input_val in self.config.test_cases:
            try:
                expected = self.config.input_output_mapping[input_val]
                # Create fresh interpreter for each test to avoid state issues
                interpreter = BrainfuckInterpreter()
                
                # Convert input to character, handling small integers properly
                if input_val == 0:
                    # Special handling for 0 input - use actual null character
                    input_char = '\x00'
                else:
                    input_char = chr(input_val)
                
                result = interpreter.run(individual.code, input_char)

                # Dynamic penalties per test (timeouts, I/O behavior)
                dynamic_penalty = 0.0
                
                # Enforce exactly one output character
                if len(result) == 1:
                    actual = ord(result[0])
                elif len(result) == 0:
                    actual = 0
                    dynamic_penalty += 25.0  # Heavy penalty for no output
                else:
                    actual = ord(result[0])
                    dynamic_penalty += 15.0 * (len(result) - 1)  # Penalty for extra outputs

                # Check input reads per test
                reads = getattr(interpreter, 'input_reads', None)
                if reads is not None and reads != 1:
                    dynamic_penalty += 20.0 * abs(reads - 1)

                # Runtime penalty
                if getattr(interpreter, 'hit_step_limit', False):
                    dynamic_penalty += 50.0  # Heavy penalty for infinite loops

                # Track accuracy (exact matches only)
                is_correct = (actual == expected)
                if is_correct:
                    correct_count += 1

                # Calculate base fitness with gradual scoring
                if is_correct:
                    base_fitness = 100.0
                else:
                    # Gradual fitness based on how close the answer is
                    diff = abs(actual - expected)
                    if diff <= 1:
                        base_fitness = 75.0
                    elif diff <= 2:
                        base_fitness = 50.0
                    elif diff <= 5:
                        base_fitness = 25.0
                    elif diff <= 10:
                        base_fitness = 10.0
                    else:
                        base_fitness = 0.0

                # Apply penalties (length penalty once at end)
                case_fitness = base_fitness - dynamic_penalty - (static_penalty / len(self.config.test_cases))
                case_fitness = max(0.0, case_fitness)  # Don't go below 0

                total_fitness += case_fitness
                individual.test_results.append(TestResult(
                    input_val=input_val,
                    expected=expected,
                    actual=actual,
                    fitness=case_fitness
                ))

            except Exception as e:
                expected = self.config.input_output_mapping.get(input_val, 0)
                individual.test_results.append(TestResult(
                    input_val=input_val,
                    expected=expected,
                    actual=0,
                    fitness=0.0,
                    error=str(e)
                ))

        # Calculate fitness (gradual scoring with penalties) and accuracy (exact matches only)
        individual.fitness = total_fitness / len(self.config.test_cases)
        individual.accuracy = (correct_count / len(self.config.test_cases)) * 100.0
        
        # Track failure cases for future sampling bias (only once per generation per test case)
        if self.config.use_dynamic_sampling:
            for result in individual.test_results:
                if result.actual != result.expected:
                    inp = result.input_val
                    # Only increment failure count once per generation per test case
                    if inp not in getattr(self, '_failure_updates_this_gen', set()):
                        self.failed_cases_history[inp] = self.failed_cases_history.get(inp, 0.0) + 1.0
                        if not hasattr(self, '_failure_updates_this_gen'):
                            self._failure_updates_this_gen = set()
                        self._failure_updates_this_gen.add(inp)
        
        return individual.fitness
    
    def _setup_validation_set(self) -> None:
        """Create a separate validation set for PERFECT detection."""
        
        # Use lambda function for validation if available
        if self.config.target_function is not None:
            self._setup_lambda_validation_set()
            return
            
        # Legacy: Use input pool for validation
        if not self.config.input_pool_mapping:
            return
            
        pool_items = list(self.config.input_pool_mapping.items())
        validation_size = min(self.config.validation_set_size, len(pool_items))
        
        # Sample validation set (different from training samples)
        validation_items = random.sample(pool_items, validation_size)
        self.config.validation_set_mapping = dict(validation_items)
        
        self.log(f"Validation set created: {list(self.config.validation_set_mapping.keys())}")
    
    def _setup_lambda_validation_set(self) -> None:
        """Create validation set from lambda function."""
        min_input, max_input = self.config.input_range
        
        # Generate all possible inputs and filter valid ones
        possible_inputs = list(range(min_input, max_input + 1))
        valid_inputs = []
        
        for inp in possible_inputs:
            try:
                output = self.config.target_function(inp)
                if 0 <= output <= self.config.max_output_value:
                    valid_inputs.append(inp)
            except:
                continue
        
        if len(valid_inputs) == 0:
            self.log("⚠️ No valid inputs for validation set")
            return
        
        # Sample validation inputs (larger set for thorough testing)
        validation_size = min(self.config.validation_samples, len(valid_inputs))
        validation_inputs = random.sample(valid_inputs, validation_size)
        
        # Generate validation set
        validation_items = {}
        for inp in validation_inputs:
            try:
                output = self.config.target_function(inp)
                if 0 <= output <= self.config.max_output_value:
                    validation_items[inp] = output
            except:
                continue
        
        self.config.validation_set_mapping = validation_items
        self.log(f"Lambda validation set created: {len(validation_items)} cases from input range {self.config.input_range}")
    
    def _validate_on_holdout(self, individual: Individual) -> bool:
        """Test individual on validation set to confirm PERFECT status."""
        if not self.config.validation_set_mapping:
            # If no validation set, fall back to training accuracy
            return individual.accuracy >= 100.0
            
        correct_count = 0
        total_count = len(self.config.validation_set_mapping)
        
        for input_val, expected in self.config.validation_set_mapping.items():
            try:
                interpreter = BrainfuckInterpreter()
                
                if input_val == 0:
                    input_char = '\x00'
                else:
                    input_char = chr(input_val)
                
                result = interpreter.run(individual.code, input_char)
                
                if len(result) == 1:
                    actual = ord(result[0])
                elif len(result) == 0:
                    actual = 0
                else:
                    actual = ord(result[0])
                
                if actual == expected:
                    correct_count += 1
                    
            except Exception:
                pass  # Count as incorrect
        
        validation_accuracy = (correct_count / total_count) * 100.0
        self.log(f"Validation check: {correct_count}/{total_count} correct ({validation_accuracy:.1f}%)")
        
        return validation_accuracy >= 100.0

    def evolve_generation(self) -> str:
        """Evolve one generation and return status."""
        # Resample test cases for this generation
        self.resample_test_cases()
        
        # Evaluate fitness for all individuals
        for individual in self.population:
            self.evaluate_fitness(individual)

        # Sort by accuracy first, then fitness (best first)
        self.population.sort(key=lambda x: (x.accuracy, x.fitness), reverse=True)

        # Track best individual (prioritize accuracy, then fitness for tie-breaking)
        current_best = self.population[0]
        if (not self.best_ever or 
            current_best.accuracy > self.best_ever.accuracy or
            (current_best.accuracy == self.best_ever.accuracy and current_best.fitness > self.best_ever.fitness)):
            self.best_ever = current_best.clone()
            self.best_accuracy_ever = self.best_ever.accuracy

            # Check for perfect solution with validation set confirmation
            if self.best_accuracy_ever >= 100.0:
                # Validate on holdout set to prevent false positives
                if self._validate_on_holdout(self.best_ever):
                    self.log("🎉 PERFECT SOLUTION FOUND! 🎉")
                    self.log(f"Final code: {self.best_ever.code}")
                    self.log("All test cases AND validation set passed with 100% accuracy!")
                    return 'PERFECT'
                else:
                    self.log(f"⚠️ Training perfect ({self.best_accuracy_ever:.1f}%) but validation failed - continuing...")

        # Create new generation
        new_population = []
        elite_count = max(1, int(self.config.population_size * self.config.elite_ratio))

        # Elite selection (keep best individuals)
        for i in range(elite_count):
            elite = self.population[i].clone()
            elite.generation_created = self.generation + 1
            new_population.append(elite)

        # Tournament selection and reproduction with controlled duplicates
        if self.config.strict_uniqueness:
            # Original strict uniqueness approach
            existing_codes = set(ind.code for ind in new_population)  # Track existing genomes
            max_attempts = self.config.population_size * 3  # Limit attempts to avoid infinite loops
            attempts = 0
            
            while len(new_population) < self.config.population_size and attempts < max_attempts:
                parent1 = self.tournament_select()
                parent2 = self.tournament_select()
                child = self.crossover(parent1, parent2)
                
                # Use current (possibly annealed) mutation rate
                temp_config = self.config
                temp_config.mutation_rate = self.current_mutation_rate
                child.mutate(temp_config)
                child.generation_created = self.generation + 1
                
                # Only add if genome is unique
                if child.code not in existing_codes:
                    new_population.append(child)
                    existing_codes.add(child.code)
                
                attempts += 1
        else:
            # Capped duplicates approach (better selection pressure)
            genome_counts = {}
            for ind in new_population:
                genome_counts[ind.code] = genome_counts.get(ind.code, 0) + 1
            
            max_attempts = self.config.population_size * 2  # Fewer attempts needed
            attempts = 0
            
            while len(new_population) < self.config.population_size and attempts < max_attempts:
                parent1 = self.tournament_select()
                parent2 = self.tournament_select()
                child = self.crossover(parent1, parent2)
                
                # Use current (possibly annealed) mutation rate
                temp_config = self.config
                temp_config.mutation_rate = self.current_mutation_rate
                child.mutate(temp_config)
                child.generation_created = self.generation + 1
                
                # Allow if under duplicate limit
                current_count = genome_counts.get(child.code, 0)
                if current_count < self.config.max_duplicates_per_genome:
                    new_population.append(child)
                    genome_counts[child.code] = current_count + 1
                
                attempts += 1
        
        # Fill remaining slots with random individuals if needed
        while len(new_population) < self.config.population_size:
            random_individual = Individual(max_length=self.config.max_program_length)
            random_individual.generation_created = self.generation + 1
            
            # Validate structure before adding
            valid_structure = (random_individual.code.startswith(',') and 
                             random_individual.code.endswith('.') and 
                             random_individual.code.count('.') == 1 and
                             random_individual.code.count(',') >= 1)  # At least one input, can have more
            
            if valid_structure:
                if self.config.strict_uniqueness:
                    if random_individual.code not in existing_codes:
                        new_population.append(random_individual)
                        existing_codes.add(random_individual.code)
                else:
                    # Check capped duplicates for random individuals too
                    current_count = genome_counts.get(random_individual.code, 0) 
                    if current_count < self.config.max_duplicates_per_genome:
                        new_population.append(random_individual)
                        genome_counts[random_individual.code] = current_count + 1

        self.population = new_population
        self.generation += 1
        
        # Clear failure update tracking for next generation
        if hasattr(self, '_failure_updates_this_gen'):
            self._failure_updates_this_gen.clear()
        
        # Update mutation rate for annealing
        if self.config.anneal_mutations:
            progress = self.generation / self.config.max_generations
            self.current_mutation_rate = (
                self.config.initial_mutation_rate * (1 - progress) + 
                self.config.final_mutation_rate * progress
            )
        
        return 'CONTINUE'

    def tournament_select(self) -> Individual:
        """Select an individual using tournament selection."""
        tournament = random.sample(self.population,
                                 min(self.config.tournament_size, len(self.population)))
        return max(tournament, key=lambda x: (x.accuracy, x.fitness))

    def _extract_body(self, code: str) -> Optional[str]:
        """Extract body from ,<body>. structure."""
        if len(code) >= 2 and code.startswith(',') and code.endswith('.'):
            return code[1:-1]
        return None

    def _is_valid_brackets_str(self, s: str) -> bool:
        """Check if brackets are balanced without modifying."""
        depth = 0
        for ch in s:
            if ch == '[': 
                depth += 1
            elif ch == ']':
                if depth == 0: 
                    return False
                depth -= 1
        return depth == 0

    def crossover(self, parent1: Individual, parent2: Individual) -> Individual:
        """Create offspring through body-only crossover that preserves ,<body>. structure."""
        if random.random() >= self.config.crossover_rate:
            return random.choice([parent1, parent2]).clone()

        # Extract bodies from both parents
        b1, b2 = self._extract_body(parent1.code), self._extract_body(parent2.code)
        if b1 is None or b2 is None:
            # Fall back: clone fitter parent
            return max([parent1, parent2], key=lambda x: (x.accuracy, x.fitness)).clone()

        # Crossover on bodies only (no bracket "repair")
        if random.random() < 0.7:
            # Single-point crossover on bodies
            i = random.randint(0, len(b1))
            k = random.randint(0, len(b2))
            new_body = b1[:i] + b2[k:]
        else:
            # Two-point crossover on bodies
            if len(b1) >= 4 and len(b2) >= 4:
                s1 = random.randint(0, len(b1)//2)
                e1 = random.randint(s1+2, len(b1))
                s2 = random.randint(0, len(b2)//2)
                new_body = b1[:s1] + b2[s2:s2+(e1-s1)] + b1[e1:]
            else:
                i, k = random.randint(0, len(b1)), random.randint(0, len(b2))
                new_body = b1[:i] + b2[k:]

        # Reconstruct program: ,<new_body>.
        new_code = ',' + new_body + '.'
        
        # Enforce length & bracket validity strictly (reject/resample if invalid)
        if len(new_code) > self.config.max_program_length or not self._is_valid_brackets_str(new_body):
            return random.choice([parent1, parent2]).clone()

        child = Individual(max_length=self.config.max_program_length)
        child.code = new_code
        return child

    def get_stats(self) -> Dict[str, float]:
        """Get current population statistics."""
        if not self.population:
            return {
                'avg_fitness': 0.0,
                'success_rate': 0.0,
                'best_fitness': 0.0,
                'best_accuracy': 0.0,
                'avg_accuracy': 0.0
            }

        avg_fitness = sum(ind.fitness for ind in self.population) / len(self.population)
        avg_accuracy = sum(ind.accuracy for ind in self.population) / len(self.population)
        success_count = sum(1 for ind in self.population if ind.accuracy >= 100.0)  # Perfect accuracy
        success_rate = (success_count / len(self.population)) * 100
        best_fitness = max(ind.fitness for ind in self.population)
        best_accuracy = max(ind.accuracy for ind in self.population)

        return {
            'avg_fitness': avg_fitness,
            'success_rate': success_rate,
            'best_fitness': best_fitness,
            'best_accuracy': best_accuracy,
            'avg_accuracy': avg_accuracy
        }

    def save_successful_genomes(self, force_save: bool = False) -> int:
        """Save successful genomes to the repository."""
        if not self.genome_repository or not self.config.save_successful_genomes:
            return 0

        saved_count = 0

        for individual in self.population:
            if individual.test_results:
                # Calculate accuracy
                correct_count = sum(1 for r in individual.test_results if r.actual == r.expected)
                total_count = len(individual.test_results)
                accuracy = (correct_count / total_count) * 100 if total_count > 0 else 0

                # Save if meets criteria
                if accuracy >= self.config.min_accuracy_to_save or force_save:
                    self.genome_repository.add_genome(
                        code=individual.code,
                        fitness=individual.fitness,
                        accuracy=accuracy,
                        function_name=self.config.function_name,
                        test_cases=self.config.test_cases,
                        expected_outputs=self.config.expected_outputs,
                        generation_found=individual.generation_created,
                        task_id=f"{self.config.function_name}_{self.generation}",
                        metadata={
                            'population_size': self.config.population_size,
                            'mutation_rate': self.config.mutation_rate,
                            'generation': self.generation
                        }
                    )
                    saved_count += 1

        if saved_count > 0:
            self.genome_repository.save_repository()
            self.log(f"Saved {saved_count} genomes to repository")

        return saved_count

    def log(self, message: str) -> None:
        """Add a message to the evolution log."""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self.evolution_log.append(log_entry)
        print(log_entry)


class EvolutionRunner:
    """High-level runner for the evolution process."""

    def __init__(self, config: EvolutionConfig = None):
        self.config = config or EvolutionConfig()
        self.engine = EvolutionEngine(self.config)

    def run_evolution(self, interactive: bool = True) -> Dict[str, Any]:
        """Run the complete evolution process."""
        self.engine.log(f"🧬 Starting Brainfuck Evolution: {self.config.function_name} 🧬")
        self.engine.log(f"Population: {self.config.population_size}, "
                       f"Mutation Rate: {self.config.mutation_rate}, "
                       f"Test Cases: {self.config.test_cases}")
        self.engine.log(f"Target Function: {self.config.function_name}")
        self.engine.log(f"Input→Output Mapping: {dict(list(self.config.input_output_mapping.items())[:6])}{'...' if len(self.config.input_output_mapping) > 6 else ''}")

        # Initialize population
        self.engine.initialize_population()

        start_time = time.time()
        perfect_found = False

        try:
            for gen in range(self.config.max_generations):
                # Evolve one generation
                result = self.engine.evolve_generation()

                # Get current stats
                stats = self.engine.get_stats()

                # Check for perfect solution
                if result == 'PERFECT':
                    perfect_found = True
                    break

                # Add delay if specified
                if self.config.generation_delay_ms > 0:
                    time.sleep(self.config.generation_delay_ms / 1000.0)

        except KeyboardInterrupt:
            self.engine.log("Evolution interrupted by user")

        # Final results
        end_time = time.time()
        duration = end_time - start_time

        final_stats = self.engine.get_stats()
        self.engine.log(f"Evolution completed in {duration:.2f} seconds")
        self.engine.log(f"Final generation: {self.engine.generation}")
        self.engine.log(f"Best fitness achieved: {self.engine.best_accuracy_ever:.2f}%")

        # Calculate and report final accuracy
        if self.engine.best_ever and self.engine.best_ever.test_results:
            correct_count = sum(1 for r in self.engine.best_ever.test_results if r.actual == r.expected)
            total_count = len(self.engine.best_ever.test_results)
            final_accuracy = (correct_count / total_count) * 100 if total_count > 0 else 0
            self.engine.log(f"Best accuracy: {correct_count}/{total_count} correct ({final_accuracy:.1f}%)")

        if perfect_found:
            self.engine.log("🏆 Perfect solution found!")

        # Save successful genomes to repository
        if self.engine.config.save_successful_genomes:
            saved_count = self.engine.save_successful_genomes(force_save=perfect_found)
            if saved_count > 0:
                self.engine.log(f"💾 Saved {saved_count} genomes to repository")

        return {
            'success': perfect_found,
            'generations': self.engine.generation,
            'duration': duration,
            'best_fitness': self.engine.best_accuracy_ever,
            'best_accuracy': final_stats.get('best_accuracy', 0),
            'final_stats': final_stats,
            'best_code': self.engine.best_ever.code if self.engine.best_ever else None
        }